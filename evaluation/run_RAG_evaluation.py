# run_RAG_evaluation.py
import os
import sys
import sqlite3
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import time
from eval_answer import AnswerEvaluator
from apis.llm_api import LLMAPI
import pandas as pd

test_num = 3

def run_step1_data():
    # æ­¥é©ŸäºŒï¼šå¾ Guest.db ä¸­æ’ˆå‡º ID, user_query, ai_response å­˜æˆ csvï¼Œæ›´åç‚º ID, Question, Answer_{llm_option}
    userDB_path = "data/user/Guest/Guest.db"
    output_dir = "mockdata"

    # ç¢ºä¿è¼¸å‡ºè³‡æ–™å¤¾å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)

    # é€£ç·šè³‡æ–™åº«
    conn = sqlite3.connect(userDB_path)
    cursor = conn.cursor()

    # æ’ˆå‡ºæ‰€æœ‰æœ‰ ai_response çš„ç´€éŒ„
    cursor.execute("""
        SELECT id, llm_option, user_query, ai_response
        FROM chat_history
        WHERE ai_response IS NOT NULL AND TRIM(ai_response) != ''
    """)
    rows = cursor.fetchall()

    if not rows:
        print("âš ï¸ ç„¡è³‡æ–™å¯åŒ¯å‡ºï¼Œè«‹ç¢ºèª chat_history è³‡æ–™æ˜¯å¦å­˜åœ¨ AI å›ç­”")
        return

    # æ ¹æ“š llm_option åˆ†çµ„åŒ¯å‡º
    data_by_model = {}
    for row in rows:
        id, llm_option, user_query, ai_response = row
        if llm_option not in data_by_model:
            data_by_model[llm_option] = []
        data_by_model[llm_option].append([id, user_query, ai_response])

    for llm_option, data in data_by_model.items():
        # ä¿®æ”¹æ¬„ä½åç¨±ç‚º Answer_{llm_option}
        df = pd.DataFrame(data, columns=["ID", "Question", f"Answer_{llm_option}"])
        model_dir = os.path.join(output_dir, llm_option)
        os.makedirs(model_dir, exist_ok=True)
        output_path = os.path.join(model_dir, f"test{test_num}_{llm_option}_response.csv")
        df.to_csv(output_path, index=False, encoding="utf-8-sig")
        print(f"âœ… å·²åŒ¯å‡ºæ¨¡å‹å›ç­”ï¼š{output_path}")

    conn.close()

def run_step2_evaluate_answers():
    evaluator_llm = "Gemma3_27b"
    llm_option = "Gemma3_27b"
    llm_input_file = f"mockdata/{llm_option}/test{test_num}_{llm_option}_response.csv"
    output_path = f"mockdata/{llm_option}/test{test_num}_{llm_option}_evaluate_{evaluator_llm}.csv"
    if os.path.exists(llm_input_file):
        print(f"ğŸ” é–‹å§‹è©•ä¼°æ¨¡å‹å›ç­”ï¼š{llm_option}")
        AnswerEvaluator(
            correct_file="mockdata/QAData_1819.csv",
            llm_input_file=llm_input_file,
            output_file=output_path,
            mode="å…§éƒ¨LLM",
            llm_option=llm_option,
            evaluator_llm=evaluator_llm,
            evaluation_attempts=3
        ).evaluate_answers()
    else:
        print(f"âŒ æ‰¾ä¸åˆ°æª”æ¡ˆï¼Œç•¥éè©•ä¼°ï¼š{llm_input_file}")

if __name__ == "__main__":
    # Step 2ï¼šé‡å°æ¯å€‹æ¨¡å‹ç”¢ç”Ÿå›ç­”
    run_step1_data()

    # Step 3ï¼šå°ç”¢ç”Ÿçš„å›ç­”é€²è¡Œæ¨¡å‹è‡ªå‹•è©•ä¼°
    run_step2_evaluate_answers()
