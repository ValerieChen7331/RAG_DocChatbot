# answer_evaluator.py
import os
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import pandas as pd
import sqlite3
import json
from tqdm import tqdm
from apis.llm_api import LLMAPI
import time
from datetime import datetime

class AnswerEvaluator:
    def __init__(self, correct_file: str, llm_input_file: str, output_file: str,
                 mode: str, llm_option: str, evaluator_llm: str = "Gemma3_27b", evaluation_attempts: int = 3):
        # åˆå§‹åŒ–è©•ä¼°å™¨ï¼Œè¨­å®šæ‰€éœ€åƒæ•¸
        self.correct_file = correct_file  # æ­£ç¢ºç­”æ¡ˆä¾†æºæª”æ¡ˆ
        self.llm_input_file = llm_input_file  # LLM å›ç­”ä¾†æºæª”æ¡ˆ
        self.output_file = output_file  # è©•ä¼°çµæœå„²å­˜ä½ç½®
        self.llm_option = llm_option  # è¢«è©•ä¼°çš„æ¨¡å‹åç¨±
        self.evaluation_attempts = evaluation_attempts  # è©•ä¼°æ¬¡æ•¸

        # åˆå§‹åŒ–è©•ä¼°ä½¿ç”¨çš„ LLMï¼ˆå¯èˆ‡ç”¢ç”Ÿå›ç­”çš„æ¨¡å‹ä¸åŒï¼‰
        self.llm = LLMAPI.get_llm(mode, evaluator_llm)

        # è©•ä¼°ä½¿ç”¨çš„æç¤ºè©ç¯„æœ¬
        self.prompt_template = """
        è«‹æ¯”è¼ƒä»¥ä¸‹å…§å®¹ï¼Œä¸¦å›ç­”ã€Œ*tã€(ä»£è¡¨å›ç­”æ­£ç¢º)æˆ–ã€Œ*fã€(ä»£è¡¨å›ç­”éŒ¯èª¤)ï¼š
        1. æª¢æŸ¥ã€Œå¯¦éš›å›æ‡‰ã€ä¸­ï¼Œæ˜¯å¦åŒ…å«ã€Œæ­£ç¢ºç­”æ¡ˆã€ä¸­çš„å¿…è¦è³‡è¨Šå’Œæ ¸å¿ƒå…§å®¹ã€‚
        2. æª¢æŸ¥é€£çµé‡‘é¡æ•¸å­—æ˜¯å¦æ­£ç¢ºã€‚
        3. æª¢æŸ¥é€£çµ url æ˜¯å¦æ­£ç¢ºã€‚
        4. ä¸æ‹˜æ³¥æ–¼æ–‡å­—å½¢å¼ï¼Œç¢ºä¿æ„æ€ä¸€è‡´ã€‚è‹¥ã€Œå¯¦éš›å›æ‡‰ã€ä¸­æœ‰æ€è€ƒéç¨‹ï¼Œå¯å¿½ç•¥ä¸åˆ—å…¥è©•ä¼°ï¼Œä»¥çµè«–ç‚ºä¸»ã€‚
        5. è«‹åˆ—å‡ºæ€è€ƒéç¨‹ï¼Œä¸¦çµ¦å‡ºçµè«–ï¼Œæœ€å¾Œè¼¸å‡º: final_answer:ã€Œ*tã€æˆ–ã€Œ*fã€ã€‚
        ---
        (1.) å•é¡Œ: {query}
        (2.) æ­£ç¢ºç­”æ¡ˆï¼š{expected_response}
        (3.) å¯¦éš›å›æ‡‰ï¼š{generated_response}
        """

    def evaluate_answers(self):
        # è®€å–æ­£ç¢ºç­”æ¡ˆèˆ‡ LLM å›ç­”çš„è³‡æ–™è¡¨
        df_llm = pd.read_csv(self.llm_input_file)
        df_correct = pd.read_csv(self.correct_file)

        # åˆä½µå…©è¡¨æ ¼ï¼Œä»¥ ID ç‚ºä¸»éµè£œä¸Šæ­£ç¢ºç­”æ¡ˆæ¬„ä½
        merged_df = pd.merge(df_llm, df_correct[['ID', 'Answer']], on='ID', how='left')

        # æª¢æŸ¥æ˜¯å¦å­˜åœ¨ç›®æ¨™å›ç­”æ¬„ä½ï¼ˆä»¥æ¨¡å‹åç¨±å‘½åï¼‰
        test_column = f"Answer_{self.llm_option}"
        if test_column not in merged_df.columns:
            raise ValueError(f"âŒ æ¬„ä½ä¸å­˜åœ¨ï¼š{test_column}ï¼Œè«‹ç¢ºèª llm_input_file æ˜¯å¦æ­£ç¢ºã€‚")

        # å¤šè¼ªè©•ä¼°ï¼Œæ¯è¼ªè©•ä¼°æ‰€æœ‰å•é¡Œ
        evaluations = []
        for attempt in range(self.evaluation_attempts):
            scores = []
            for idx, row in tqdm(
                    merged_df.iterrows(),
                    desc=f"ç¬¬ {attempt + 1} è¼ªè©•ä¼°ä¸­ï¼š{self.llm_option}",
                    unit="é¡Œ",
                    leave=True,
                    dynamic_ncols=True
            ):
                # åŸ·è¡Œå–®ç­†å›ç­”çš„è©•ä¼°
                scores.append(
                    self._evaluate_single_response(row['Question'], row['Answer'], row[test_column])
                )
                time.sleep(3)
            evaluations.append(scores)
            # è·‘å¤šè¼ªï¼Œä¸­é–“ä¼‘æ¯
            current_time = datetime.now().strftime('%H:%M:%S')
            print(f"âœ… å®Œæˆæ¨¡å‹å›ç­”è©•ä¼°ï¼š{self.llm_option}ï¼ˆæ™‚é–“ï¼š{current_time}ï¼‰ï¼Œæš«åœ 5 åˆ†é˜é¿å…é™æµ")
            time.sleep(250)


        # è¨ˆç®—å¹³å‡å¾—åˆ†èˆ‡æ˜¯å¦æ­£ç¢º
        avg_scores = [sum(score_set)/len(score_set) for score_set in zip(*evaluations)]
        merged_df['SimilarityScore'] = avg_scores
        merged_df['SimilarityBoolean'] = [score > 0.5 for score in avg_scores]

        # é¡¯ç¤ºè©•ä¼°ç¸½çµè³‡è¨Š
        correct_count = sum(merged_df['SimilarityBoolean'])
        total_count = len(merged_df)
        average_score = sum(avg_scores) / total_count if total_count > 0 else 0
        print("ğŸ“Š è©•ä¼°ç¸½çµï¼š")
        print(f"âœ”ï¸ correct_similarity æ­£ç¢ºå›ç­”æ•¸é‡ï¼š{correct_count} / {total_count}")
        print(f"â­ å¹³å‡ç›¸ä¼¼åº¦åˆ†æ•¸ï¼š{average_score:.2f}")
        # å„²å­˜çµ±è¨ˆçµæœ
        summary_path = self.output_file.replace(".csv", "_summary.csv")
        df = pd.DataFrame([{
            "CorrectCount": correct_count,
            "TotalCount": total_count,
            "AverageScore": average_score
        }])
        df.to_csv(summary_path, index=False, encoding='utf-8-sig')

        # å„²å­˜çµæœç‚º CSV
        merged_df.to_csv(self.output_file, index=False, encoding='utf-8-sig')
        # å„²å­˜è‡³ SQLite
        self._save_to_db(merged_df)

        # åªè·‘ä¸€è¼ª
        # current_time = datetime.now().strftime('%H:%M:%S')
        # print(f"âœ… å®Œæˆæ¨¡å‹å›ç­”è©•ä¼°ï¼š{self.llm_option}ï¼ˆæ™‚é–“ï¼š{current_time}ï¼‰ï¼Œæš«åœ 5 åˆ†é˜é¿å…é™æµ")
        # time.sleep(300)

    def _evaluate_single_response(self, query, expected_response, generated_response) -> int:
        # å°‡è¼¸å…¥ä»£å…¥æç¤ºè©æ¨¡æ¿
        prompt = self.prompt_template.format(
            query=query,
            expected_response=expected_response,
            generated_response=generated_response
        )
        try:
            # å‘¼å«è©•å¯© LLM å–å¾—å›æ‡‰çµæœ
            result = self.llm.invoke(prompt).strip().lower()
            # print("ğŸ§ª LLM å›å‚³åŸå§‹å…§å®¹ï¼š", result)
            final = result.split('final_answer:')[-1].strip()
            return 1 if '*t' in final else 0
        except Exception as e:
            print("âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š", e)
            return 0

    def _save_to_db(self, df):
        # å„²å­˜è©•ä¼°çµæœè‡³ SQLite è³‡æ–™åº«ï¼ˆæ¬„ä½èˆ‡ merged_df å®Œå…¨ä¸€è‡´ï¼‰
        db_path = self.output_file.replace('.csv', '.db')
        with sqlite3.connect(db_path) as conn:
            # è‹¥æœ‰ Docs æ¬„ä½ï¼Œå…ˆè½‰ç‚º JSON å­—ä¸²æ ¼å¼
            if 'Docs' in df.columns:
                df['Docs'] = df['Docs'].apply(
                    lambda x: json.dumps(x, ensure_ascii=False) if isinstance(x, (dict, list)) else x
                )

            # å°‡æ¬„ä½ ID æ”¹åç‚º question_id ä»¥é¿å…èˆ‡ä¸»éµ id è¡çª
            if 'ID' in df.columns:
                df = df.rename(columns={'ID': 'question_id'})

            # è‡ªå‹•å¯«å…¥ DataFrameï¼ˆè‹¥è³‡æ–™è¡¨ä¸å­˜åœ¨æœƒè‡ªå‹•å»ºç«‹ï¼‰
            try:
                df.to_sql('evaluations', conn, if_exists='append', index=False)
                print(f"ğŸ“ å·²å¯«å…¥ SQLiteï¼š{db_path}")
            except Exception as e:
                print("âŒ å¯«å…¥ SQLite ç™¼ç”ŸéŒ¯èª¤ï¼š", e)