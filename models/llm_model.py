# llm_model.py

from apis.llm_api import LLMAPI
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.prompts import ChatPromptTemplate
from langchain.prompts import PromptTemplate
import re
from typing import List, Dict

# é è¨­æ˜¯å¦é–‹å•Ÿ query_llm_direct çš„èŠå¤©è¨˜æ†¶åŠŸèƒ½
DEFAULT_USE_CHAT_HISTORY: bool = False


class LLMModel:
    def __init__(self, chat_session_data):
        """
        åˆå§‹åŒ– LLMModel ç‰©ä»¶ï¼Œå„²å­˜ä½¿ç”¨è€…å°è©± session è¨­å®šã€‚
        """
        self.chat_session_data = chat_session_data
        self.mode = chat_session_data.get('mode')
        self.llm_option = chat_session_data.get('llm_option')

    def query_llm_direct(self, query: str) -> str:
        """
        æŸ¥è©¢ LLM å›æ‡‰ä½¿ç”¨è€…è¼¸å…¥çš„å•é¡Œï¼Œ
        å¯ä¾è¨­å®šæ±ºå®šæ˜¯å¦ä½¿ç”¨æ­·å²å°è©±ä½œç‚ºè¨˜æ†¶ã€‚
        """
        # æ ¹æ“šæ¨¡å¼èˆ‡æ¨¡å‹é¸é …å–å¾— LLM ç‰©ä»¶
        llm = LLMAPI.get_llm(self.mode, self.llm_option)

        # ğŸ” æ˜¯å¦ä½¿ç”¨æ­·å²å°è©±è¨˜æ†¶ï¼ˆå„ªå…ˆå– session ä¸­è¨­å®šï¼Œå…¶æ¬¡å–é è¨­å¸¸æ•¸ï¼‰
        use_memory = self.chat_session_data.get("use_memory", DEFAULT_USE_CHAT_HISTORY)

        if use_memory:
            # âœ… é–‹å•ŸèŠå¤©è¨˜æ†¶åŠŸèƒ½
            active_window_index = self.chat_session_data.get('active_window_index', 0)
            memory_key = f'conversation_memory_{active_window_index}'

            # å¦‚æœå°šæœªå»ºç«‹å°æ‡‰è¨˜æ†¶é«”å‰‡åˆå§‹åŒ–
            if memory_key not in self.chat_session_data:
                self.chat_session_data[memory_key] = ConversationBufferMemory(
                    memory_key="history", input_key="input"
                )

            # å°‡éå¾€å°è©±ç´€éŒ„è¼‰å…¥ chat memory
            chat_history_data = self.chat_session_data.get('chat_history', [])
            if chat_history_data:
                chat_history = ChatMessageHistory()
                for record in chat_history_data:
                    chat_history.add_user_message(record['user_query'])
                    chat_history.add_ai_message(record['ai_response'])
                self.chat_session_data[memory_key].chat_memory = chat_history

            # å»ºç«‹ Prompt æ¨¡æ¿ï¼ˆå« historyï¼‰
            init_prompt = """
            ä½ æ˜¯ä¸€ä½è¦ªåˆ‡ã€å°ˆæ¥­ä¸”åæ‡‰å¿«é€Ÿçš„å•ç­”åŠ©ç†ï¼Œè«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼ˆå°ç£ç”¨èªï¼‰å›è¦†ã€‚ 
            ä»¥ä¸‹æ˜¯å°è©±ç´€éŒ„ï¼š  
            {history}
            è«‹æ ¹æ“šä¸Šè¿°å…§å®¹ï¼Œé‡å°ä»¥ä¸‹å•é¡Œï¼Œæä¾›ç°¡å–®æ˜ç­ã€æ­£ç¢ºå¯¦ç”¨çš„å›ç­”ï¼Œå¿…è¦æ™‚å¯é©ç•¶è£œå……èªªæ˜ï¼Œä½†é¿å…éé•·æˆ–è´…è¿°ï¼š  
            {input}
            """

            # å»ºç«‹ ConversationChain ä¸¦åŸ·è¡ŒæŸ¥è©¢
            conversation_chain = ConversationChain(
                llm=llm,
                memory=self.chat_session_data[memory_key],
                prompt=ChatPromptTemplate.from_template(init_prompt)
            )
            result = conversation_chain.invoke(input=query)
            response = result.get('response', '')

        else:
            # ğŸš« ä¸ä½¿ç”¨èŠå¤©è¨˜æ†¶ï¼Œå–®è¼ªå•ç­”æ¨¡å¼
            prompt_template = PromptTemplate.from_template("""
            ä½ æ˜¯ä¸€ä½è¦ªåˆ‡ã€å°ˆæ¥­ä¸”åæ‡‰å¿«é€Ÿçš„å•ç­”åŠ©ç†ï¼Œè«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼ˆå°ç£ç”¨èªï¼‰å›è¦†ã€‚  
            ä»¥ä¸‹æ˜¯ä½¿ç”¨è€…çš„å•é¡Œï¼š  
            {input}  
            è«‹ç°¡æ˜ä¸”æ­£ç¢ºåœ°å›ç­”ã€‚
            """)
            formatted_prompt = prompt_template.format(input=query)
            result = llm.invoke(formatted_prompt)

            # è™•ç†ä¸åŒ LLM å›å‚³æ ¼å¼
            response = result if isinstance(result, str) else getattr(result, 'content', '')

        return response

    def set_window_title(self, query: str) -> str:
        """
        ä½¿ç”¨ LLM æ ¹æ“šä½¿ç”¨è€…å•é¡Œï¼Œè‡ªå‹•ç”¢ç”Ÿç°¡çŸ­çš„è¦–çª—æ¨™é¡Œã€‚
        """
        try:
            llm = LLMAPI.get_llm(self.mode, self.llm_option)
            prompt_template = self._title_prompt()
            formatted_prompt = prompt_template.format(query=query)
            title = llm.invoke(formatted_prompt)

            # è™•ç†å¤–éƒ¨ LLM å›å‚³æ ¼å¼
            if self.chat_session_data.get('mode') != 'å…§éƒ¨LLM':
                title = title.content

            # éæ¿¾ç‰¹æ®Šç¬¦è™Ÿï¼Œåªä¿ç•™ä¸­è‹±æ•¸èˆ‡ç©ºç™½
            title = re.sub(r"[^\u4e00-\u9fffA-Za-z0-9 ]", "", title)

            # æœ€å¤šå–å‰ 12 å­—
            if len(title) > 12:
                title = title[:12]

            # å­˜å…¥ session
            self.chat_session_data['title'] = title
            return title

        except Exception as e:
            print(f"ç”Ÿæˆè¦–çª—æ¨™é¡Œæ–‡å­—æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return "æœªç”¢ç”Ÿæ¨™é¡Œ"

    def _title_prompt(self):
        """
        å›å‚³è¦–çª—æ¨™é¡Œç”Ÿæˆçš„ Prompt æ¨¡æ¿ã€‚
        """
        template = """
        æ ¹æ“šä»¥ä¸‹æå•(Q:)ï¼Œåˆ—å‡º1å€‹é—œéµå­—ä½œç‚ºæ¨™é¡Œã€‚è«‹éµå®ˆä»¥ä¸‹è¦å‰‡ï¼š
        1. åªèƒ½è¼¸å‡ºæ¨™é¡Œé—œéµå­—ï¼Œä¸å¯æœ‰å¤šé¤˜èªªæ˜ã€‚
        2. æ¨™é¡Œè«‹é™åˆ¶åœ¨12å­—ä»¥å…§ã€‚
        ---
        Q: {query}        
        """
        return PromptTemplate(input_variables=["query"], template=template)

    def doc_summary(self, doc) -> str:
        """
        å°å–®ä»½æ–‡ä»¶å…§å®¹é€²è¡Œæ‘˜è¦ï¼Œå›å‚³ä¸€æ®µç´„300å­—çš„æ‘˜è¦ã€‚
        """
        try:
            llm = LLMAPI.get_llm(self.mode, self.llm_option)
            prompt_template = self._doc_summary_prompt()
            formatted_prompt = prompt_template.format(documents=doc.page_content)
            doc_summary = llm.invoke(formatted_prompt)

            # è™•ç†å¤–éƒ¨ LLM æ ¼å¼
            if self.chat_session_data.get('mode') != 'å…§éƒ¨LLM':
                doc_summary = doc_summary.content

            return doc_summary.strip()

        except Exception as e:
            print(f"doc_summary æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return ""

    def _doc_summary_prompt(self):
        """
        å›å‚³æ‘˜è¦ä»»å‹™çš„ Prompt æ¨¡æ¿ã€‚
        """
        template = """
        è«‹æ ¹æ“šä¸‹æ–¹æ–‡ç« å…§å®¹æ’°å¯«æ‘˜è¦ï¼Œå»ºè­°é•·åº¦ç´„ç‚º 300 å­—ã€‚
        æ‘˜è¦å…§å®¹é ˆåŒ…å«ï¼š
        1. æ–‡ä»¶çš„ä¸»è¦åŠŸèƒ½èˆ‡æ ¸å¿ƒç›®çš„
        2. å‡ºç¾çš„å°ˆæœ‰åè©èˆ‡ç°¡è¦è§£é‡‹
        3. å„ç« ç¯€æˆ–æ®µè½çš„çµæ§‹èˆ‡é‡é»æ¦‚è¦½
        
        è«‹é¿å…æ·»åŠ ç„¡æ ¹æ“šçš„æ¨è«–ï¼Œåƒ…æ ¹æ“šæä¾›çš„å…§å®¹é€²è¡Œæ­¸ç´ã€‚
        ---
        æ–‡ç« å…§å®¹ï¼š
        {documents}      
        """
        return PromptTemplate(input_variables=["documents"], template=template)
