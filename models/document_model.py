import tempfile
from langchain_chroma import Chroma
from apis.file_paths import FilePaths
from apis.embedding_api import EmbeddingAPI
import os
from langchain.text_splitter import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter
import logging
from pathlib import Path
from typing import List
# from langchain.document_loaders import PyMuPDFLoader as PyMuPDF4LLMLoader, TextLoader
from langchain_community.document_loaders import PyMuPDFLoader as PyMuPDF4LLMLoader, TextLoader
from langchain.schema import Document

# é—œé–‰ Chroma é ç«¯é™æ¸¬è³‡æ–™æ”¶é›†ï¼ˆä¿è­·ä½¿ç”¨è€…éš±ç§ï¼‰
os.environ["CHROMA_TELEMETRY"] = "False"

# è¨­å®šæ—¥èªŒè¨˜éŒ„çš„ç´šåˆ¥ç‚º INFO
logging.basicConfig(level=logging.INFO)

class DocumentModel:
    def __init__(self, chat_session_data):
        # åˆå§‹åŒ– hat_session_data
        self.chat_session_data = chat_session_data
        # åˆå§‹åŒ–æ–‡ä»¶è·¯å¾‘
        self.file_paths = FilePaths()
        username = self.chat_session_data.get("username")
        conversation_id = self.chat_session_data.get("conversation_id")
        self.tmp_dir = self.file_paths.get_tmp_dir(username, conversation_id)
        self.vector_store_dir = self.file_paths.get_local_vector_store_dir(username, conversation_id)

    from typing import Tuple

    def create_temporary_files(self, source_doc) -> Tuple[str, str]:
        """æ ¹æ“šä¸Šå‚³æª”æ¡ˆé¡å‹ï¼ˆpdf / mdï¼‰å»ºç«‹è‡¨æ™‚æ–‡ä»¶ä¸¦è¿”å› (è‡¨æ™‚æª”å, åŸå§‹æª”å)ã€‚"""
        self.tmp_dir.mkdir(parents=True, exist_ok=True)

        file_ext = source_doc.get('type', 'pdf')  # é è¨­ç‚º pdfï¼Œå¦‚æœæœ‰ type å­—æ®µå°±ç”¨ type
        suffix = f'.{file_ext}'

        with tempfile.NamedTemporaryFile(delete=False, dir=self.tmp_dir.as_posix(), suffix=suffix) as tmp_file:
            tmp_file.write(source_doc['content'])  # å¯«å…¥æ–‡ä»¶å…§å®¹
            tmp_name = Path(tmp_file.name).name
            org_name = source_doc['name']
            logging.info(f"Created temporary file: {tmp_name}")
        return tmp_name, org_name

    def load_documents(self) -> Document:
        """å¾è‡¨æ™‚ç›®éŒ„ä¸­åŠ è¼‰ PDF å’Œ MD æ–‡ä»¶"""
        self._pdf_to_md()
        document = self._load_md_file()
        return document

    def _pdf_to_md(self):
        """
        è¼‰å…¥ PDFï¼Œè½‰ç‚º Markdownï¼Œä¸¦å„²å­˜åœ¨ç›¸åŒç›®éŒ„
        """
        for pdf in self.tmp_dir.rglob("*.pdf"):
            try:
                logging.info(f"ğŸ”„ è¼‰å…¥ PDFï¼š{pdf.name}")
                loader = PyMuPDF4LLMLoader(pdf.as_posix())
                docs = loader.load()

                # å„²å­˜ç‚º Markdown
                md_path = self.tmp_dir / f"{pdf.stem}.md"
                try:
                    with open(md_path, "w", encoding="utf-8") as f:
                        for doc in docs:
                            f.write(doc.page_content + "\n\n---\n\n")
                    logging.info(f"ğŸ“ å·²å„²å­˜ PDF è½‰ Markdownï¼š{md_path.name}")
                except Exception as e:
                    logging.warning(f"âš ï¸ å„²å­˜ Markdown å¤±æ•—ï¼š{md_path.name}ï¼ŒéŒ¯èª¤ï¼š{e}")
            except Exception as e:
                logging.warning(f"âŒ PDF è¼‰å…¥å¤±æ•—ï¼š{pdf.name}ï¼ŒéŒ¯èª¤ï¼š{e}", exc_info=True)

    def _load_md_file(self) -> Document:
        """
        è¼‰å…¥å–®ä¸€ Markdown æª”æ¡ˆå…§å®¹ä¸¦å›å‚³ Document
        """
        md_files = list(self.tmp_dir.rglob('*.md'))
        if not md_files:
            raise FileNotFoundError("â— æ‰¾ä¸åˆ°ä»»ä½• Markdown (.md) æª”æ¡ˆã€‚")

        md_file = md_files[0]  # åªè™•ç†ç¬¬ä¸€å€‹æª”æ¡ˆ

        try:
            loader = TextLoader(md_file.as_posix(), encoding='utf-8')
            md_documents = loader.load()

            if not md_documents:
                raise ValueError("â— æ‰¾åˆ° Markdown æª”æ¡ˆï¼Œä½†è®€å–å…§å®¹ç‚ºç©ºã€‚")

            logging.info(f"âœ… å·²è¼‰å…¥ Markdown æ–‡ä»¶ï¼š{md_file.name}")
            return md_documents[0]  # åªå–ç¬¬ä¸€ä»½æ–‡ä»¶

        except Exception as e:
            logging.warning(f"âš ï¸ è¼‰å…¥ Markdown æ–‡ä»¶å¤±æ•—ï¼š{md_file.name}ï¼ŒéŒ¯èª¤ï¼š{e}")
            raise e

    def delete_temporary_files(self):
        # åˆªé™¤è‡¨æ™‚æ–‡ä»¶
        for file in self.tmp_dir.iterdir():
            try:
                logging.info(f"Deleting temporary file: {file}")
                file.unlink()
            except Exception as e:
                logging.error(f"Error deleting file {file}: {e}")

    def chunk_document_with_md(self, document: Document, org_name: str, header_depth: int = 3):
        """
        å°‡ Markdown æ–‡æª”ä¾ç…§æ¨™é¡Œèˆ‡å­—å…ƒç´šæ‹†åˆ†æˆå¤šå€‹å° chunkã€‚
        æ”¯æ´è‡ªå®šç¾© Markdown æ¨™é¡Œæ·±åº¦ï¼ˆä¾‹å¦‚ Header 1 ~ Header 6ï¼‰ã€‚
        æ¯å€‹ chunk éƒ½æœƒé™„å¸¶ metadata èˆ‡åŸå§‹æª”æ¡ˆåç¨±ã€‚
        """

        document_chunks = []  # ç”¨ä¾†æš«å­˜æ¯å€‹æ‹†åˆ†å‡ºä¾†çš„å°æ®µè½

        # 1. å‹•æ…‹å»ºç«‹ Markdown æ¨™é¡Œå±¤ç´šï¼ˆä¾‹å¦‚: [('#', 'Header 1'), ..., ('######', 'Header 6')])
        headers_to_split_on = [(f"{'#' * i}", f"Header {i}") for i in range(1, header_depth + 1)]

        # 2. ä½¿ç”¨ MarkdownHeaderTextSplitter æŒ‰æ¨™é¡Œå€å¡Šæ‹†åˆ†
        markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)

        # å‚³å…¥ç´”æ–‡å­—å…§å®¹åšæ‹†åˆ†ï¼ˆä¸èƒ½æ˜¯æ•´å€‹ Document ç‰©ä»¶ï¼‰
        md_header_splits = markdown_splitter.split_text(document.page_content)

        # 3. ä½¿ç”¨éè¿´å¼å­—å…ƒåˆ‡å‰²å™¨é€²è¡Œæ®µè½å…§éƒ¨ç´°æ‹†
        chunk_size = 800  # æ¯å€‹å°å¡Šçš„æœ€å¤§å­—å…ƒæ•¸
        chunk_overlap = 300  # æ¯å¡Šä¹‹é–“çš„é‡ç–Šå­—æ•¸
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)

        # å°æ¯å€‹ Markdown æ¨™é¡Œæ®µè½åšé€²ä¸€æ­¥åˆ‡å‰²
        for header_group in md_header_splits:
            # æ‹†æˆæ›´å°çš„ chunk
            splits = text_splitter.split_text(header_group.page_content)

            for split in splits:
                # å°‡æ‹†å‡ºä¾†çš„å°æ®µè½èˆ‡ metadata ä¸€èµ·å­˜èµ·ä¾†
                document_chunks.append({
                    "page_content": split,
                    "metadata": header_group.metadata,
                    "org_name": org_name
                })

        # 4. å°‡æ‰€æœ‰å°æ®µè½è½‰æ›æˆ LangChain çš„ Document æ ¼å¼
        transformed_documents = []
        for i, chunk in enumerate(document_chunks):
            # å°‡åŸå§‹æª”åèˆ‡æ¨™é¡Œ metadata åˆä½µé€² metadataï¼Œä¸¦åŠ ä¸Š page ç·¨è™Ÿ
            transformed_documents.append(
                Document(
                    metadata={**chunk["metadata"], 'chunk_id': i+1, 'org_name': chunk["org_name"]},
                    page_content=chunk["org_name"] + "\n" + str(chunk["metadata"]) + "\n" + chunk["page_content"]
                )
            )

        # è¨˜éŒ„æˆåŠŸè³‡è¨Š
        logging.info(f"âœ… æˆåŠŸå°‡æ–‡æª”æ‹†åˆ†ç‚º {len(transformed_documents)} å€‹å¡Šã€‚")

        return transformed_documents

    def embed_into_vectordb(self, document_chunks):
        """
        å°‡å·²æ‹†åˆ†çš„æ–‡ä»¶å€å¡Šï¼ˆchunksï¼‰åµŒå…¥è‡³æœ¬åœ°å‘é‡è³‡æ–™åº«ï¼ˆä½¿ç”¨ Chromaï¼‰ï¼Œä»¥ä¾¿å¾ŒçºŒæª¢ç´¢ä½¿ç”¨ã€‚
        """
        # å¾ chat_session_data ä¸­å–å¾—ç”¨æˆ¶æŒ‡å®šçš„åµŒå…¥æ¨¡å¼
        mode = self.chat_session_data.get("mode")
        embedding = self.chat_session_data.get("embedding")

        # é€éåµŒå…¥ API å–å¾—å°æ‡‰çš„åµŒå…¥å‡½å¼ï¼ˆembedding functionï¼‰
        embedding_function = EmbeddingAPI.get_embedding_function(mode, embedding)

        # è‹¥æ–‡ä»¶å€å¡Šç‚ºç©ºï¼Œæ‹‹å‡ºéŒ¯èª¤æç¤º
        if not document_chunks:
            raise ValueError("â— æ‰¾ä¸åˆ°æ–‡ä»¶å€å¡Šå¯ä¾›åµŒå…¥ã€‚è«‹ç¢ºèªæ–‡ä»¶æ˜¯å¦å·²æˆåŠŸåˆ‡åˆ†ã€‚")

        # ä½¿ç”¨ Chroma å‘é‡è³‡æ–™åº«ï¼Œå°‡æ–‡ä»¶å€å¡ŠåµŒå…¥ä¸¦å„²å­˜è‡³æœ¬åœ°ç£ç¢Ÿ
        Chroma.from_documents(
            documents=document_chunks,  # è¦åµŒå…¥çš„ LangChain Document ç‰©ä»¶æ¸…å–®
            embedding=embedding_function,  # ä½¿ç”¨çš„åµŒå…¥å‡½å¼
            persist_directory=self.vector_store_dir.as_posix()  # æœ¬åœ°å‘é‡è³‡æ–™åº«å„²å­˜è·¯å¾‘
        )

        # è¨˜éŒ„æˆåŠŸè¨Šæ¯ï¼šè¡¨ç¤ºè³‡æ–™å·²æˆåŠŸåµŒå…¥ä¸¦å„²å­˜
        logging.info(f"âœ… å‘é‡è³‡æ–™åº«å·²æˆåŠŸå„²å­˜æ–¼ï¼š{self.vector_store_dir}")

