import os
import sqlite3
import pandas as pd
from datetime import datetime
from langchain_chroma import Chroma
from langchain_core.runnables import RunnableWithMessageHistory, RunnableLambda
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.output_parsers import StrOutputParser

from apis.llm_api import LLMAPI
from apis.embedding_api import EmbeddingAPI
from apis.file_paths import FilePaths
from models.database_base import BaseDB  # âœ… å¼•å…¥å…±ç”¨çš„è³‡æ–™åº«æ“ä½œåŸºç¤é¡åˆ¥

# é—œé–‰ Chroma é™æ¸¬åŠŸèƒ½ä»¥ä¿è­·ä½¿ç”¨è€…è³‡æ–™éš±ç§
os.environ["CHROMA_TELEMETRY"] = "False"

class RAGModel:
    def __init__(self, chat_session_data):
        """
        åˆå§‹åŒ– RAG æ¨¡å‹ã€‚
        - å„²å­˜ä½¿ç”¨è€… session è¨­å®š
        - æº–å‚™å‘é‡å„²å­˜èˆ‡è³‡æ–™åº«è·¯å¾‘
        """
        self.chat_session_data = chat_session_data
        self.mode = chat_session_data.get("mode")
        self.llm_option = chat_session_data.get("llm_option")

        file_paths = FilePaths()
        self.output_dir = file_paths.get_output_dir()
        username = chat_session_data.get("username")
        conversation_id = chat_session_data.get("conversation_id")
        self.vector_store_dir = file_paths.get_local_vector_store_dir(username, conversation_id)

    def query_llm_rag(self, query, doc_summary):
        """
        ä¸»æŸ¥è©¢å‡½æ•¸ï¼šæ¥æ”¶ä½¿ç”¨è€…å•é¡Œèˆ‡æ–‡ä»¶æ‘˜è¦ï¼ŒåŸ·è¡Œ RAG æµç¨‹ï¼Œå›å‚³å›ç­”èˆ‡æ–‡æª”ã€‚
        - åŒ…å« query é‡å¯«ï¼ˆä½¿ç”¨ä¸Šä¸‹æ–‡ï¼‰
        - çµåˆæª¢ç´¢æ–‡æª”èˆ‡ LLM å›ç­”
        - å°‡æŸ¥è©¢æ­·å²èˆ‡æ–‡æª”å­˜å…¥è³‡æ–™åº«
        """
        try:
            # åˆå§‹åŒ– LLM èˆ‡å‘é‡åµŒå…¥æ¨¡å‹
            llm = LLMAPI.get_llm(self.mode, self.llm_option)
            embedding = self.chat_session_data.get("embedding")
            embedding_function = EmbeddingAPI.get_embedding_function('å…§éƒ¨LLM', embedding)

            # å»ºç«‹ Chroma å‘é‡è³‡æ–™åº«ä¸¦åˆå§‹åŒ–æª¢ç´¢å™¨
            vector_db = Chroma(
                embedding_function=embedding_function,
                persist_directory=self.vector_store_dir.as_posix()
            )
            retriever = vector_db.as_retriever(search_type="mmr", search_kwargs={"k": 5, "fetch_k": 10})

            # å‰µå»ºå¯é‡å¯« query çš„æª¢ç´¢å™¨ï¼ˆåŠ ä¸Šæ­·å²èˆ‡æ‘˜è¦ï¼‰
            history_aware_retriever = self._rewrite_query_and_history_aware_retriever(llm, retriever, doc_summary)

            # å»ºç«‹å°è©±å‹ RAG chain
            conversational_rag_chain = self._create_conversational_rag_chain(llm, history_aware_retriever)

            # åŸ·è¡ŒæŸ¥è©¢éˆï¼ˆåŒ…å«èŠå¤©æ­·å²ï¼‰
            result_rag = conversational_rag_chain.invoke({
                'input': query,
                'chat_history': self._get_chat_history_from_session()
            })

            # æ‹†è§£å›å‚³çµæœ
            response = result_rag.get('answer', '')
            retrieved_documents = result_rag.get('context', [])

            # å†æ¬¡å–å¾—é‡å¯«å¾Œçš„æŸ¥è©¢æ–‡å­—ï¼ˆæ–¹ä¾¿è¨˜éŒ„ï¼‰
            prompt = self._build_rewrite_prompt(doc_summary)
            question_generator = prompt | llm | StrOutputParser()
            rewritten_query = question_generator.invoke({
                "input": query,
                "chat_history": self._get_chat_history_from_session().messages
            })

            print("1. retrieved_documents: ", retrieved_documents)

            return response, retrieved_documents, rewritten_query

        except Exception as e:
            print(f"æŸ¥è©¢ query_llm_rag æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return "", []

    def _build_rewrite_prompt(self, doc_summary: str):
        """
        å»ºç«‹ Query é‡å¯«æç¤ºæ¨¡æ¿ï¼Œè®“ LLM èƒ½æ ¹æ“šèŠå¤©æ­·å²èˆ‡æ–‡ä»¶æ‘˜è¦ç”¢å‡ºæ›´æ˜ç¢ºçš„æŸ¥è©¢ã€‚
        """
        return ChatPromptTemplate.from_messages([
            ("system", f"""
                æ ¹æ“šèŠå¤©è¨˜éŒ„å’Œæ–‡ä»¶æ‘˜è¦ï¼Œé‡æ–°è¡¨è¿°ä½¿ç”¨è€…æå•ã€‚\
                è©²æå•å¯èƒ½åƒè€ƒèŠå¤©è¨˜éŒ„ä¸­çš„ä¸Šä¸‹æ–‡ï¼Œè«‹å°‡å…¶é‡æ§‹ç‚ºä¸€å€‹å¯ä»¥ç¨ç«‹ç†è§£çš„å•é¡Œã€‚\
                ä¸è¦å›ç­”å•é¡Œï¼Œåªéœ€é‡æ–°è¡¨è¿°ã€‚\
                æ–‡ä»¶æ‘˜è¦: {doc_summary}
            """),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ])

    def _rewrite_query_and_history_aware_retriever(self, llm, retriever, doc_summary: str):
        """
        åŒ…è£ retrieverï¼šåœ¨æª¢ç´¢å‰è‡ªå‹•å‘¼å« LLM é‡å¯« queryã€‚
        å›å‚³ç¬¦åˆ Runnable æ ¼å¼çš„ç‰©ä»¶ä»¥ä¾¿æ•´åˆé€² chain ä¸­ã€‚
        """
        prompt = self._build_rewrite_prompt(doc_summary)
        question_generator = prompt | llm | StrOutputParser()

        def retriever_with_rewritten_query(inputs):
            rewritten = question_generator.invoke({
                "input": inputs["input"],
                "chat_history": inputs["chat_history"]
            })
            print(f"ğŸ“ [æŸ¥è©¢é‡å¯«] åŸå§‹æŸ¥è©¢: {inputs['input']}")
            print(f"ğŸ“ [æŸ¥è©¢é‡å¯«] é‡å¯«å¾ŒæŸ¥è©¢: {rewritten}")
            return retriever.invoke({"query": rewritten})

        return RunnableLambda(retriever_with_rewritten_query)

    def _create_conversational_rag_chain(self, llm, history_aware_retriever):
        """
        å»ºç«‹å°è©±å‹ RAG Chainï¼šçµåˆ retriever èˆ‡å•ç­”éˆï¼Œä¸¦ä¿æœ‰èŠå¤©æ­·å²ã€‚
        """
        qa_prompt = ChatPromptTemplate.from_messages([
            ("system", """
                æ‚¨æ˜¯å›ç­”å•é¡Œçš„åŠ©æ‰‹ã€‚
                ä½¿ç”¨ä»¥ä¸‹æª¢ç´¢åˆ°çš„å…§å®¹ä¾†å›ç­”å•é¡Œï¼Œè«‹åˆ¤æ–·ä½¿ç”¨è€…çš„å•é¡Œéœ€è¦ç”±å“ªåˆ†æ–‡æª”ä¾†å›ç­”ï¼Œä¸ä¸€å®šæ¯ä»½æ–‡æª”éƒ½ä½¿ç”¨åˆ°ã€‚
                è«‹å°å‡ºåŸæ–‡èˆ‡é ç¢¼ï¼Œå†é€²è¡Œèªªæ˜ï¼Œå›ç­”ä½¿ç”¨è€…å•é¡Œã€‚
                è‹¥ç„¡æ³•å¾æª¢ç´¢å…§å®¹å¾—åˆ°ç­”æ¡ˆï¼Œè«‹èª å¯¦å›ç­”ã€Œæˆ‘ä¸çŸ¥é“ã€ã€‚ä¸å¾—æä¾›è‡†æ¸¬æˆ–ç„¡ä¾æ“šçš„ç­”æ¡ˆï¼
                {context}
            """),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ])
        qa_chain = create_stuff_documents_chain(llm, qa_prompt)
        rag_chain = create_retrieval_chain(history_aware_retriever, qa_chain)

        return RunnableWithMessageHistory(
            rag_chain,
            self._get_chat_history_from_session,
            input_messages_key="input",
            history_messages_key="chat_history",
            output_messages_key="answer",
        )

    def _get_chat_history_from_session(self) -> ChatMessageHistory:
        """
        å¾ chat_session_data ä¸­å–å¾—ä½¿ç”¨è€…å°è©±æ­·å²ã€‚
        è‹¥å°šæœªå„²å­˜ä»»ä½•æ­·å²ï¼Œå‰‡å›å‚³ç©ºç´€éŒ„ã€‚
        """
        chat_history_data = self.chat_session_data.get('chat_history', [])
        chat_history = ChatMessageHistory()
        for record in chat_history_data:
            chat_history.add_user_message("")
            chat_history.add_ai_message("")
        return chat_history