# -*- coding: utf-8 -*-
"""
RAGModel â€’ æª¢ç´¢å¢å¼·ç”Ÿæˆæ¨¡å‹ï¼ˆæ¨¡çµ„åŒ–ç‰ˆæœ¬ï¼‰
================================================
åŠŸèƒ½
----
* é€éå‘é‡è³‡æ–™åº«æª¢ç´¢ (cosine / MMR / Crossâ€‘Encoder rerank) èˆ‡ LLM ç”Ÿæˆï¼Œå¯¦ç¾ RAG å•ç­”ã€‚
* æ”¯æ´ QueryÂ Rewriteã€MMR èˆ‡Â Rerankerï¼Œå¯ç”± `RAGConfig` å½ˆæ€§é–‹é—œã€‚
* SingletonÂ è¼‰å…¥ CrossEncoderï¼Œä¸¦æ–¼æŸ¥è©¢å¾Œè‡ªå‹•é‡‹æ”¾ CUDA è¨˜æ†¶é«”ã€‚

ä½¿ç”¨æ–¹å¼
--------
```python
from rag_model import RAGModel, RAGConfig

config = RAGConfig(
    use_rewrite=True,      # æ˜¯å¦å•Ÿç”¨æŸ¥è©¢é‡å¯«
    use_mmr=True,          # æ˜¯å¦å•Ÿç”¨ MMR å¤šæ¨£æ€§
    use_reranker=False,    # æ˜¯å¦å•Ÿç”¨ CrossEncoder rerank
    cosine_k=20,
    mmr_k=15,
    rerank_k=5,
)

rag = RAGModel(chat_session_data, config)
answer, docs, rewritten = rag.query_llm_rag(query, doc_summary)
```
"""

import os
import gc
from typing import Dict, List, Tuple, Optional

import torch
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain_core.runnables import RunnableWithMessageHistory, RunnableLambda
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from sentence_transformers import CrossEncoder
from langchain_community.chat_message_histories import ChatMessageHistory


from apis.llm_api import LLMAPI
from apis.embedding_api import EmbeddingAPI
from apis.file_paths import FilePaths

# é—œé–‰ Chroma é™æ¸¬
os.environ["CHROMA_TELEMETRY"] = "False"

# ---------------------------------------------------------------------------
# RAGConfig â€’  æ‰€æœ‰è¶…åƒé›†ä¸­é…ç½®
# ---------------------------------------------------------------------------
class RAGConfig:
    """é›†ä¸­ç®¡ç† RAG ç›¸é—œåƒæ•¸ï¼Œæ–¹ä¾¿å¤–éƒ¨æ³¨å…¥æˆ–å‹•æ…‹èª¿æ•´ã€‚"""

    def __init__(
        self,
        *,
        use_rewrite: bool = True,
        use_mmr: bool = False,
        use_reranker: bool = False,
        cosine_k: int = 5,
        mmr_k: int = 5,
        rerank_k: int = 5,
        batch_size: int = 4,
    ) -> None:
        self.use_rewrite = use_rewrite
        self.use_mmr = use_mmr
        self.use_reranker = use_reranker
        self.cosine_k = cosine_k
        self.mmr_k = mmr_k
        self.rerank_k = rerank_k
        self.batch_size = batch_size


# ---------------------------------------------------------------------------
# CrossEncoder Singleton Â (é¿å…é‡è¤‡è¼‰å…¥å¤§æ¨¡å‹)
# ---------------------------------------------------------------------------
_CROSS_ENCODER: Optional[CrossEncoder] = None

def get_cross_encoder(model_name: str = "BAAI/bge-reranker-v2-m3") -> CrossEncoder:
    global _CROSS_ENCODER
    if _CROSS_ENCODER is None:
        device = "cuda" if torch.cuda.is_available() else "cpu"
        try:
            _CROSS_ENCODER = CrossEncoder(model_name, device=device)
        except RuntimeError:
            _CROSS_ENCODER = CrossEncoder(model_name, device="cpu")
    return _CROSS_ENCODER


# ---------------------------------------------------------------------------
# RAGModel ä¸»é¡åˆ¥
# ---------------------------------------------------------------------------
class RAGModel:
    """æª¢ç´¢å¢å¼·ç”Ÿæˆæ¨¡å‹ (Retrievalâ€‘Augmented Generation)ã€‚"""

    # æ˜¯å¦åœ¨æŸ¥è©¢ä¸­ä½¿ç”¨èŠå¤©æ­·å² (å…¨åŸŸé è¨­ï¼Œå¯ç”± session è¦†å¯«)
    DEFAULT_USE_CHAT_HISTORY: bool = False

    def __init__(self, chat_session_data: Dict, config: Optional[RAGConfig] = None):
        # ----- æœƒè©±èˆ‡è·¯å¾‘ -----
        self.chat_session_data = chat_session_data
        self.mode = chat_session_data.get("mode")
        self.llm_option = chat_session_data.get("llm_option")
        file_paths = FilePaths()
        username = chat_session_data.get("username")
        conversation_id = chat_session_data.get("conversation_id")
        self.vector_store_dir = file_paths.get_local_vector_store_dir(username, conversation_id)

        # ----- é…ç½® -----
        self.config = config or RAGConfig()
        self.batch_size = self.config.batch_size
        self.reranker = get_cross_encoder() if self.config.use_reranker else None
        self._last_rewritten_query: str = ""

    # ---------------------------------------------------------------------
    # Public API
    # ---------------------------------------------------------------------
    def query_llm_rag(self, query: str, doc_summary: str) -> Tuple[str, List[Document], str]:
        """ä¸»æµç¨‹ï¼šé‡å¯«â†’æª¢ç´¢â†’ç”Ÿæˆã€‚"""
        try:
            # 1. å–å¾— LLM èˆ‡ Embedding function
            llm = LLMAPI.get_llm(self.mode, self.llm_option)
            embed_fn = EmbeddingAPI.get_embedding_function(self.mode, self.chat_session_data.get("embedding"))

            # 2. å‘é‡è³‡æ–™åº«
            self.vector_db = Chroma(
                embedding_function=embed_fn,
                persist_directory=self.vector_store_dir.as_posix(),
            )

            # 3. å»ºç«‹ retriever (å«é‡å¯« / rerank)
            retriever = self._build_rewrite_retrieve_rerank_chain(llm, doc_summary)

            # 4. å»ºç«‹ Conversational RAG chain
            rag_chain = self._build_conversational_rag_chain(llm, retriever)

            # 5. åŸ·è¡Œ
            result = rag_chain.invoke({
                "input": query,
                "chat_history": self._get_chat_history_from_session(),
            })

            answer = result.get("answer", "")
            docs = result.get("context", [])
            return answer, docs, self._last_rewritten_query
        finally:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                gc.collect()

    # ------------------------------------------------------------------
    # Chain Builders
    # ------------------------------------------------------------------
    def _build_rewrite_retrieve_rerank_chain(self, llm, doc_summary: str) -> RunnableLambda:
        """æŠŠ Rewriteã€Cosineã€MMRã€Rerank ä¸²ç‚ºåŒä¸€å€‹ retrieverã€‚"""

        # ---- (A) QueryÂ Rewrite Prompt â†’ LLM â†’ parser ----
        rewrite_prompt = self._build_rewrite_prompt(doc_summary)
        rewrite_chain = rewrite_prompt | llm | StrOutputParser()

        # ---- (B) å…§éƒ¨å‡½å¼ï¼šrewrite â†’ retrieve â†’ rerank ----
        def _inner(inputs):
            origin_query = inputs["input"]
            chat_history = inputs["chat_history"]

            # 1) æ˜¯å¦é‡å¯«
            if self.config.use_rewrite:
                rewritten = rewrite_chain.invoke({
                    "input": origin_query,
                    "chat_history": chat_history,
                }).strip() or origin_query
            else:
                rewritten = origin_query
            self._last_rewritten_query = rewritten

            # 2) Cosine æª¢ç´¢
            cosine_docs = self.vector_db.similarity_search(rewritten, k=self.config.cosine_k)

            # 3) MMRï¼ˆå¯é¸ï¼‰
            if self.config.use_mmr:
                mmr_docs = self.vector_db.max_marginal_relevance_search(
                    rewritten, k=self.config.mmr_k, fetch_k=self.config.cosine_k)
            else:
                mmr_docs = cosine_docs

            # 4) Rerankï¼ˆå¯é¸ï¼‰
            if self.config.use_reranker and self.reranker:
                if len(mmr_docs) <= self.config.rerank_k:
                    final_docs = mmr_docs
                else:
                    pairs = [(rewritten, d.page_content) for d in mmr_docs]
                    scores = self.reranker.predict(pairs, batch_size=self.batch_size)
                    ranked = sorted(zip(scores, mmr_docs), key=lambda p: p[0], reverse=True)[: self.config.rerank_k]
                    final_docs = [d for _, d in ranked]
            else:
                final_docs = mmr_docs

            return final_docs

        return RunnableLambda(_inner)

    def _build_conversational_rag_chain(self, llm, retriever):
        qa_prompt = ChatPromptTemplate.from_messages([
            ("system", """
            ä½ æ˜¯çŸ¥è­˜å•ç­”åŠ©æ‰‹ã€‚ä»¥ä¸‹æ˜¯ç›¸é—œæ–‡ä»¶å…§å®¹ï¼Œè«‹æ“·å–å¿…è¦æ®µè½å¾Œä½œç­”ã€‚
            è‹¥å…§å®¹ä¸è¶³ä»¥å›ç­”ï¼Œè«‹èª å¯¦å›è¦†ã€Œæˆ‘ä¸çŸ¥é“ã€ã€‚
            {context}
            """),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ])
        qa_chain = create_stuff_documents_chain(llm, qa_prompt)
        rag_chain = create_retrieval_chain(retriever, qa_chain)
        return RunnableWithMessageHistory(
            rag_chain,
            self._get_chat_history_from_session,
            input_messages_key="input",
            history_messages_key="chat_history",
            output_messages_key="answer",
        )

    # ------------------------------------------------------------------
    # Prompt Builders & Helpers
    # ------------------------------------------------------------------
    def _build_rewrite_prompt(self, doc_summary: str) -> ChatPromptTemplate:
        return ChatPromptTemplate.from_messages([
            (
                "system",
                f"""
                ä½ æ˜¯ä¸€å€‹æŸ¥è©¢é‡å¯«åŠ©æ‰‹ï¼Œç›®çš„æ˜¯æ ¹æ“šèŠå¤©æ­·å²ï¼Œå°‡ä½¿ç”¨è€…å•é¡Œæ”¹å¯«æˆèƒ½ç¨ç«‹ç†è§£çš„å®Œæ•´æŸ¥è©¢å¥ã€‚
                è«‹**åƒ…é‡å¯«ï¼Œä¸è¦å›ç­”**ï¼Œä¸¦**ä¿æŒåŸå§‹èªæ„**ï¼›ä»¥èŠå¤©æ­·å²ç‚ºä¸»è¦ä¾æ“šï¼Œæ–‡ä»¶æ‘˜è¦åƒ…ä¾›è¼”åŠ©åƒè€ƒã€‚
                æ–‡ä»¶æ‘˜è¦ï¼š{doc_summary}
                """,
            ),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ])

    from langchain_community.chat_message_histories import ChatMessageHistory

    def _get_chat_history_from_session(self) -> ChatMessageHistory:
        """
        å°‡ chat_session_data["chat_history"]ï¼ˆlist[dict]ï¼‰è½‰æ›ç‚º LangChain æ‰€éœ€çš„ ChatMessageHistoryã€‚
        æ”¯æ´ä»¥ä¸‹å…©ç¨®æ ¼å¼ï¼š
        1. {"role": "user" / "assistant", "content": "..."}
        2. {"user_query": "...", "ai_response": "..."}ï¼ˆæœƒè‡ªå‹•æ‹†åˆ†ç‚º user / assistantï¼‰
        å¦‚æœæœªå•Ÿç”¨è¨˜æ†¶ï¼Œå‰‡è¿”å›ç©ºæ­·å²ã€‚
        """
        use_mem = self.chat_session_data.get("use_memory", self.DEFAULT_USE_CHAT_HISTORY)
        history = ChatMessageHistory()

        if not use_mem:
            print("ğŸ“­ Chat memory is OFFï¼ˆuse_memory=Falseï¼‰ï¼Œä¸é€å‡ºæ­·å²ç´€éŒ„")
            return history

        raw_history = self.chat_session_data.get("chat_history", [])
        # print(f"ğŸ“š Chat memory is ONï¼Œé–‹å§‹è½‰æ› {len(raw_history)} å‰‡è¨Šæ¯ç‚º ChatMessageHistory")
        # print(raw_history)

        for rec in raw_history:
            # å…¼å®¹æ ¼å¼ 1: {"role": "user", "content": "..."}
            if "role" in rec and "content" in rec:
                role = rec.get("role", "").lower()
                content = rec.get("content", "")

                if role == "user":
                    history.add_user_message(content)
                elif role in ("assistant", "ai"):
                    history.add_ai_message(content)
                else:
                    print(f"âš ï¸ ç™¼ç¾æœªçŸ¥è§’è‰²ï¼š{role}ï¼Œå…§å®¹å·²ç•¥é")

            # å…¼å®¹æ ¼å¼ 2: {"user_query": "...", "ai_response": "..."}
            elif "user_query" in rec and "ai_response" in rec:
                user_msg = rec.get("user_query", "").strip()
                ai_msg = rec.get("ai_response", "").strip()

                if user_msg:
                    history.add_user_message(user_msg)
                if ai_msg:
                    history.add_ai_message(ai_msg)

            else:
                print(f"âš ï¸ ç™¼ç¾æœªçŸ¥è¨Šæ¯æ ¼å¼ï¼š{rec}ï¼Œå…§å®¹å·²ç•¥é")

        # print("âœ… å·²è½‰æ›ç‚º ChatMessageHistoryï¼Œå‹åˆ¥ç‚ºï¼š", type(history))
        print("ğŸ“ å…§å®¹ç­†æ•¸ï¼š", len(history.messages))

        # for i, msg in enumerate(history.messages[:2]):
        #     print(f"ğŸ§¾ ç¬¬{i + 1}å‰‡è¨Šæ¯ï¼š{msg.type} / {msg.content}")

        return history

